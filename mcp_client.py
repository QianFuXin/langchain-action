import asyncio
import time

from langchain_core.messages import HumanMessage, ToolMessage, AIMessage
from langgraph.checkpoint.memory import MemorySaver
from mcp import ClientSession
from mcp.client.streamable_http import streamablehttp_client

from langgraph.prebuilt import create_react_agent
from langchain_mcp_adapters.tools import load_mcp_tools

from utils import model
config = {"configurable": {"thread_id": str(int(time.time()))}}
memory = MemorySaver()


async def chat(agent):
    print("ğŸ’¬ è¯·è¾“å…¥é—®é¢˜ï¼Œè¾“å…¥ 'exit' é€€å‡ºã€‚\n")
    while True:
        question = input("You: ").strip()
        if question.lower() in ["exit", "quit"]:
            print("ğŸ‘‹ å†è§ï¼")
            break

        try:
            async for chunk, metadata in agent.astream(
                    {"messages": [HumanMessage(question)]},
                    config,
                    stream_mode="messages"
            ):
                if isinstance(chunk, ToolMessage):
                    print("=" * 20 + "tool" + "=" * 20)
                    print(chunk)
                    print("=" * 20 + "tool" + "=" * 20)
                if isinstance(chunk, AIMessage):
                    print(chunk.content, end="")
            print()
        except Exception as e:
            print(f"âš ï¸ è°ƒç”¨å‡ºé”™: {e}\n")


async def main():
    async with streamablehttp_client("http://localhost:8001/mcp/") as (read, write, _):
        async with ClientSession(read, write) as session:
            # åˆå§‹åŒ–è¿æ¥
            await session.initialize()

            # è·å– MCP å·¥å…·
            tools = await load_mcp_tools(session)
            print(f"âœ… å·²åŠ è½½å·¥å…· {len(tools)} ä¸ª")

            # åˆ›å»º agentï¼ˆå¸¦è®°å¿†ï¼‰
            agent = create_react_agent(model, tools, checkpointer=memory)

            # è¿›å…¥äº¤äº’å¼å¾ªç¯
            await chat(agent)


if __name__ == "__main__":
    asyncio.run(main())